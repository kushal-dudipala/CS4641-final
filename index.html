<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>House Price Prediction Model</title>
    <style>
        /* Base Styling */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
    
        /* Header Styling */
        header {
            background-color: #35424a;
            color: #ffffff;
            padding: 20px 0;
            text-align: center;
        }
        /* Navigation Bar Styling */
        nav {
            background-color: #e8491d;
            color: #ffffff;
            padding: 10px;
            text-align: center;
        }

        nav a {
            color: #ffffff;
            margin: 0 15px;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s ease;
        }

        nav a:hover {
            color: #dddddd; /* Subtle hover effect for links */
        }
        /* Container for Content */
        .container {
            padding: 20px;
            max-width: 1200px;
            margin: auto;
            background-color: #ffffff;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
        }
    
        /* Section Spacing */
        section {
            margin-bottom: 40px;
        }
    
        /* Section Headings */
        h2 {
            border-bottom: 2px solid #e8491d;
            padding-bottom: 10px;
            color: #35424a;
        }
    
        /* Table Styling */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
        }
    
        table, th, td {
            border: 1px solid #dddddd;
        }
    
        th, td {
            padding: 12px;
            text-align: left;
        }
    
        th {
            background-color: #f2f2f2;
        }
    
        /* Visualization Styling */
        .visualization-container {
            display: flex;
            flex-direction: column;
            gap: 20px;
            max-width: 100%; /* Container respects page width */
            overflow: hidden;
        }
    
        /* Image and Text Row for Visualizations */
        .image-text-row {
            display: flex;
            flex-direction: row;
            align-items: start;
            gap: 20px;
            max-width: 100%;
        }
    
        /* Image Styling */
        .image-text-row img {
            width: 100%;
            max-width: 700px; /* Maintains natural width */
            height: auto;
            border-radius: 8px;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
        }
    
        /* Text Content Styling for Images */
        .text-content {
            flex: 1; /* Allows text to take the remaining space */
        }
    
        /* Responsive Design */
        @media (max-width: 768px) {
            .image-text-row {
                flex-direction: column; /* Stacks image and text vertically */
                align-items: center;
            }
    
            .image-text-row img, .text-content {
                width: 100%; /* Full width for mobile */
                max-width: none; /* Removes max-width restriction on small screens */
            }
        }
    </style>
</head>
<body>

    <header>
        <h1>House Price Prediction Model, Group 119</h1>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#problem-definition">Problem Definition</a>
        <a href="#methods">Methods</a>
        <a href="#results">Results</a>
        <a href="#references">References</a>
        <a href="#contribution">Contribution</a>
        <a href="#gantt-chart">Gantt Chart</a>
    </nav>

    <div class="container">
        
        <!-- Introduction/Background Section -->
        <section id="introduction">
            <h2>Introduction/Background</h2>
            <p>Predicting house prices is critical to the real estate industry, as it provides valuable insight for buyers, sellers, and even investors. Since we have access to housing data, we want to build a model that takes into account various home characteristics, such as square footage and home type, for accurate and reliable home price predictions.</p>

            <p>Studies such as by Kok et al. (2017) utilized a linear regression model to predict home prices with features including house size, number of rooms, and location. The model was proven accurate based on the given training dataset, and the results were easy to interpret.</p>
            
            <p>We will be using the Ames Housing Dataset, an alternative to the popular Boston Dataset found in James et al. (2018), but with more features and observations. This dataset can be found at the following link: <a href="https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data?select=data_description.txt" target="_blank">Ames Housing Dataset on Kaggle</a></p>
        </section>
        
        <!-- Problem Definition Section -->
        <section id="problem-definition">
            <h2>Problem Definition</h2>
            
            <h3>Problem</h3>
            <p>The problem we are addressing is the challenge of accurately predicting house prices located in Ames, Iowa. Traditional methods often rely on simple models or manual predictions which don’t capture the more complex relationships between predictive variables. This leads buyers, sellers, and investors to make suboptimal decisions because of a lack of accurate information.</p>
            
            <h3>Motivation</h3>
            <p>By accurately automating house price prediction, it would help homebuyers, sellers, real estate agents, and investors make informed decisions.</p>
        </section>
        
        <!-- Methods Section -->
        <section id="methods">
            <h2>Methods</h2>
            
            <h3>Data Preprocessing Methods:</h3>
            <ul>
                <li>Imputation:
                    <ul>
                        <li>Numeric Columns: Median imputation to handle skewed distributions.</li>
                        <li>Categorical Columns: Mode imputation for missing values, filling based on grouping (e.g., median for neighborhood).</li>
                    </ul>
                </li>
                <li>Encoding: One-hot encoding for categorical variables to ensure compatibility with regression models.</li>
                <li>Target Transformation: Log-transform of the target variable (SalePrice) to handle skewness.</li>
                <li>Feature Scaling: StandardScaler applied to numeric columns for uniformity in model fitting.</li>
                <li>Dimensionality Reduction: PCA applied to retain 95% of the variance while reducing feature dimensions.</li>
            </ul>
        
            <h3>Machine Learning Models:</h3>
            <ul>
                <li><strong>Lasso Regression:</strong>
                    <ul>
                        <li>Uses L1 regularization for automatic feature selection and reducing overfitting.</li>
                        <li>Hyperparameter tuning performed on alpha values ranging from 0.0001 to 1.0 using grid search.</li>
                    </ul>
                </li>
                <li><strong>Ridge Regression:</strong>
                    <ul>
                        <li>Employs L2 regularization to manage multicollinearity and provide robust predictions.</li>
                        <li>Hyperparameter tuning on alpha values similar to Lasso for optimal regularization strength.</li>
                    </ul>
                </li>
                <li><strong>Random Forest:</strong>
                    <ul>
                        <li>An ensemble learning method with multiple decision trees for robust predictions and feature importance ranking.</li>
                        <li>Hyperparameters included 100 estimators and a maximum depth of 15 to balance accuracy and overfitting.</li>
                    </ul>
                </li>
                <li><strong>Stacked Residual Model:</strong>
                    <ul>
                        <li>Combines Lasso Regression and Random Forest, with Lasso addressing linear relationships and Random Forest handling residuals.</li>
                        <li>Hyperparameter tuning performed on Lasso alpha and Random Forest parameters for optimal stacking.</li>
                    </ul>
                </li>
            </ul>
            
            <p>The preprocessing pipeline ensured data quality, consistency, and compatibility with the selected models. Lasso and Ridge Regression were chosen for their ability to regularize and manage high-dimensional data, with Ridge offering robust predictions and Lasso aiding feature selection. Random Forest provided flexibility to capture nonlinear patterns, while the Stacked Model combined these strengths to deliver the best overall performance. Hyperparameter tuning and cross-validation ensured that the models generalized well across unseen data.</p>
        </section>
        
        <!-- Results and Expectations Section -->
        <section id="results">
            <h2>Results and Expectations</h2>
            
            <h3>Model-Specific Visualizations and metrics:</h3>
            <p>Click to expand to view each model's visualizations and performance metrics.</p>
            
            <details>
                <summary>Legacy (Midterm LASSO)</summary>
                <h4>Visualizations</h4>
                <div class="visualization-container">

                    <!-- Actual vs Predicted Sale Prices -->
                    <div class="image-text-row">
                        <img src="legacy_visualizations/Actual_vs_Predicted_sales_prices.png" alt="Lasso Regression: Actual vs Predicted Sale Prices">
                        <div class="text-content">
                            <p>This image is a scatterplot displaying the relationship between the actual and predicted sale prices for the training dataset. Each point represents a house, with the actual price on the x-axis and the predicted price on the y-axis. The dashed red line represents a perfect prediction, where predicted prices match the actual prices. The scatterplot shows that the model is fairly accurate, as most points are close to this ideal line. However, there is some inaccuracy at higher price ranges, indicating limitations in the model’s performance.</p>
                        </div>
                    </div>

                    <!-- Distribution of Residuals -->
                    <div class="image-text-row">
                        <img src="legacy_visualizations/Description_of_Residuals.png" alt="Distribution of Residuals for Lasso Regression">
                        <div class="text-content">
                            <p>This image is a histogram displaying the distribution of residuals, which are the differences between actual and predicted sale prices for the training dataset. The residuals are centered around zero, indicating that the model’s predictions are generally unbiased. The spread of the distribution shows some outliers, with a slight right skew, suggesting a tendency for underprediction in higher-priced homes. The distribution of residuals provides insight into the model’s error characteristics.</p>
                        </div>
                    </div>


                </div>
            <h4>Quantitative Metrics:</h4>
                <ul>
                    <li> Moderate RMSE (Root Mean Squared Error): $41,578</li>
                    <li>High R² Score: 0.9301</li>
                </ul>
            </details>

            <details>
                <summary>Lasso Regression</summary>
                <h4>Visualizations</h4>
                <div class="visualization-container">

                    <!-- Hyperparameter Cross-Validation Results -->
                     <div class="image-text-row">
                        <img src="lasso_visualizations/lasso_alpha_vs_rmse.png" alt="Lasso Regression: Hyperparameter Cross-Validation Results">
                        <div class="text-content">
                            <p>This line plot shows the relationship between the alpha hyperparameter (on a logarithmic scale) and the RMSE. It highlights the trade-offs between model regularization strength and performance, with the optimal alpha value minimizing RMSE. Using this, the optimal alpha was determined as 0.0001 as it led to the least RMSE.</p>
                        </div>
                    </div>

                    <!-- Actual vs Predicted Sale Prices -->
                    <div class="image-text-row">
                        <img src="lasso_visualizations/lasso_comparison_actual_vs_predicted.png" alt="Lasso Regression: Actual vs Predicted Sale Prices">
                        <div class="text-content">
                            <p>This scatterplot is displaying the relationship between the actual and predicted sale prices for the training dataset. Each point represents a house, with the actual price on the x-axis and the predicted price on the y-axis. Compared to the midterm version, this model appaears to scale to higher value houses, due to the use of log space, to reduce feature covariance.</p>
                        </div>
                    </div>

                    <!-- Distribution of Residuals -->
                    <div class="image-text-row">
                        <img src="lasso_visualizations/lasso_distribution_residuals.png" alt="Distribution of Residuals for Lasso Regression">
                        <div class="text-content">
                            <p>This histogram is displaying the distribution of residuals, which are the differences between actual and predicted sale prices for the training dataset. The residuals are fairly centered around 0, indicating that the model’s predictions are generally unbiased. The spread of the distribution shows some outliers, with a slight right skew, suggesting a tendency for underprediction in higher-priced homes. The distribution of residuals provides insight into the model’s error characteristics.</p>
                        </div>
                    </div>

                    <!-- K-Fold Evaluation Results -->
                    <div class="image-text-row">
                        <img src="lasso_visualizations/lasso_kfold_rmse_boxplot.png" alt="Lasso Regression: K-Fold Evaluation Results">
                        <div class="text-content">
                            <p>This boxplot displays the Root Mean Squared Error (RMSE) values obtained across multiple folds during k-fold cross-validation. Each box represents the range of RMSE values for a specific parameter setting (e.g., alpha in regularized models). The central line within each box indicates the median RMSE, while the top and bottom edges represent the upper and lower quartiles, respectively. It highlights the consistency of the model's performance and the stability of predictions across different data splits.</p>
                        </div>
                    </div>

                    <!-- Feature Importance -->
                    <div class="image-text-row">
                        <img src="lasso_visualizations/lasso_permutation_importances.png" alt="Top 15 Features by Permutation Importance for Lasso Regression">
                        <div class="text-content">
                            <p>This bar plot displays the top 15 features ranked by their permutation importance. According to the model, the most important features to predict home price are "Overall Quality" and "Second Floor Square Footage"</p>
                        </div>
                    </div>

                </div>
            <h4>Quantitative Metrics:</h4>
                <ul>
                    <li>Best Alpha: 0.0001</li>
                    <li>Cross-Validation RMSE: 0.01</li>
                    <li>Training RMSE: 0.13</li>
                    <li>Training R² Score: 0.8946</li>
                    <li>Alpha Values Shape: (50,)</li>
                    <li>RMSE Values Shape: (50,)</li>
                </ul>
            </details>

            <details>
                <summary>Ridge Regression</summary>
                <h4>Visualizations:</h4>
                <div class="visualization-container">
            
                    <!-- Hyperparameter Cross-Validation Results -->
                    <div class="image-text-row">
                        <img src="ridge_visualizations/ridge_alpha_vs_rmse.png" alt="Ridge Regression: Hyperparameter Cross-Validation Results">
                        <div class="text-content">
                            <p>This line plot shows the relationship between the alpha hyperparameter (on a logarithmic scale) and the RMSE. It highlights the trade-offs between model regularization strength and performance, with the optimal alpha value minimizing RMSE. Using this, the optimal alpha was determined as 1.0 as it led to the least RMSE.</p>
                        </div>
                    </div>
            
                    <!-- Actual vs Predicted Sale Prices -->
                    <div class="image-text-row">
                        <img src="ridge_visualizations/ridge_actual_vs_predicted.png" alt="Ridge Regression: Actual vs Predicted Sale Prices">
                        <div class="text-content">
                            <p>This scatterplot is displaying the relationship between the actual and predicted sale prices for the training dataset. Each point represents a house, with the actual price on the x-axis and the predicted price on the y-axis. Compared to the LASSO regression, this model appaears to scale fine at higher value houses, but there is some error in the mid-price range.</p>
                        </div>
                    </div>
            
                    <!-- Distribution of Residuals -->
                    <div class="image-text-row">
                        <img src="ridge_visualizations/ridge_residuals.png" alt="Distribution of Residuals for Ridge Regression">
                        <div class="text-content">
                            <p>This histogram is displaying the distribution of residuals, which are the differences between actual and predicted sale prices for the training dataset. The residuals are fairly centered around 0, but less centered than the LASSO regression; indicating that the model’s predictions are generally more biased than LASSO. The spread of the distribution shows some outliers, with a slight right skew, suggesting a tendency for underprediction in higher-priced homes. The distribution of residuals provides insight into the model’s error characteristics.</p>
                        </div>
                    </div>
            
                    <!-- Cross-Validation RMSE Across Folds -->
                    <div class="image-text-row">
                        <img src="ridge_visualizations/ridge_cv_rmse.png" alt="Ridge Regression: Cross-Validation RMSE Across Folds">
                        <div class="text-content">
                            <p>This bar plot depicts RMSE for each cross-validation fold, showcasing the model's consistency across different data splits and highlighting areas where performance could be improved. Relative to other models, ridge regression had a very consistant RMSE.</p>
                        </div>
                    </div>
            
                    <!-- Feature Importance -->
                    <div class="image-text-row">
                        <img src="ridge_visualizations/ridge_top_features.png" alt="Top 15 Features by Coefficient Importance for Ridge Regression">
                        <div class="text-content">
                            <p>This bar plot displays the top 15 features ranked by their permutation importance. According to the model, the most important features to predict home price are "Condition2_PosN" and "RoofMatl_WdShngl"</p>
                        </div>
                    </div>
            
                </div>
                <h4>Quantitative Metrics:</h4>
                <ul>
                    <li>Best Alpha: 1.0</li>
                    <li>Cross-Validation RMSE: 0.01</li>
                    <li>Training RMSE: 0.11</li>
                    <li>Training R² Score: 0.9271</li>
                </ul>
            </details>
            
            <details>
                <summary>Random Forest</summary>
                <h4>Visualizations:</h4>
                <div class="visualization-container">
            
                    <!-- Actual vs Predicted Sale Prices -->
                    <div class="image-text-row">
                        <img src="forest_visualizations/rf_actual_vs_predicted.png" alt="Random Forest: Actual vs Predicted Sale Prices">
                        <div class="text-content">
                            <p>This scatterplot is displaying the relationship between the actual and predicted sale prices for the training dataset. Each point represents a house, with the actual price on the x-axis and the predicted price on the y-axis. Compared to other models, the points on this graph seem incredibely accurate. This is also reflected by the RMSE of 0.9816, indicating that the model is incredibely accurate in predicting home price</p>
                        </div>
                    </div>
            
                    <!-- Cross-Validation RMSE Across Folds -->
                    <div class="image-text-row">
                        <img src="forest_visualizations/rf_cv_rmse.png" alt="Random Forest: Cross-Validation RMSE Across Folds">
                        <div class="text-content">
                            <p>This bar plot visualizes RMSE for each fold in cross-validation. It demonstrates the stability of the Random Forest model’s performance across different data splits, with consistent RMSE values across folds. It is not as good as ridge regression, however.
                                NOTE: The RMSE values are "negative" due to the scoring method used in cross-validation. Please disregard the negative sign.

                            </p>
                        </div>
                    </div>
            
                    <!-- Distribution of Residuals -->
                    <div class="image-text-row">
                        <img src="forest_visualizations/rf_residuals.png" alt="Random Forest: Distribution of Residuals">
                        <div class="text-content">
                            <p>This histogram is displaying the distribution of residuals, which are the differences between actual and predicted sale prices for the training dataset. The residuals are centered around 0, ndicating that the model’s predictions are generally more biased than LASSO. The spread of the distribution shows a low amount outliers, but with a slight right skew, suggesting a tendency for underprediction in higher-priced homes. The distribution of residuals provides insight into the model’s error characteristics.</p>
                        </div>
                    </div>
            
                    <!-- Feature Importance -->
                    <div class="image-text-row">
                        <img src="forest_visualizations/rf_top_features.png" alt="Random Forest: Top 15 Most Important Features">
                        <div class="text-content">
                            <p>This bar plot displays the top 15 features ranked by their permutation importance. According to the model, the most important features to predict home price are "Overall Quality" and "GrLivArea"</p>
                        </div>
                    </div>
            
                </div>
                <h4>Quantitative Metrics:</h4>
                <ul>
                    <li>Training RMSE: 0.05</li>
                    <li>Training R² Score: 0.9816</li>
                </ul>
            </details>

            <details>
                <summary>Stacked Model</summary>
                <h4>Visualizations:</h4>
                <div class="visualization-container">
                    
                    <!-- Hyperparameter Cross-Validation Results -->
                    <div class="image-text-row">
                        <img src="stacked_visualizations/stacked_alpha_vs_rmse.png" alt="Stacked Residual Model: Alpha vs RMSE">
                        <div class="text-content">
                            <p>This line plot shows the relationship between the alpha hyperparameter (on a logarithmic scale) and the RMSE. It highlights the trade-offs between model regularization strength and performance, with the optimal alpha value minimizing RMSE. Using this, the optimal alpha was determined as 0.0001 as it led to the least RMSE.</p>
                        </div>
                    </div>

                    <!-- Actual vs Predicted Sale Prices -->
                    <div class="image-text-row">
                        <img src="stacked_visualizations/stacked_actual_vs_predicted.png" alt="Stacked Residual Model: Actual vs Predicted Sale Prices">
                        <div class="text-content">
                            <p>This scatterplot is displaying the relationship between the actual and predicted sale prices for the training dataset. Each point represents a house, with the actual price on the x-axis and the predicted price on the y-axis. Compared to other models, the points on this graph seem incredibely accurate. This is also reflected by the RMSE of 0.9866, indicating that the model is the most accurate in predicting home price</p>
                        </div>
                    </div>
            
                    <!-- Distribution of Residuals -->
                    <div class="image-text-row">
                        <img src="stacked_visualizations/stacked_residuals.png" alt="Stacked Residual Model: Distribution of Residuals">
                        <div class="text-content">
                            <p>This histogram is displaying the distribution of residuals, which are the differences between actual and predicted sale prices for the training dataset. The residuals are very centered around 0, with a low amount of outliers; indicating that the model’s predictions are generally unbaised. The spread of the distribution shows a slight right skew, suggesting a tendency for underprediction in higher-priced homes. The distribution of residuals provides insight into the model’s error characteristics.</p>
                        </div>
                    </div>

                    <!-- Cross-Validation RMSE Across Folds -->
                    <div class="image-text-row">
                        <img src="stacked_visualizations/stacked_cv_rmse.png" alt="Stacked Residual Model: Cross-Validation RMSE Across Folds">
                        <div class="text-content">
                            <p>This bar plot visualizes RMSE for each fold in cross-validation. It demonstrates the stability of the stacked model’s performance across different data splits, with consistent RMSE values across folds. It is not as good as ridge regression, however.</p>
                        </div>
                    </div>
            
                </div>
                <h4>Quantitative Metrics:</h4>
                <ul>
                    <li>Best Alpha: 0.0001</li>
                    <li>Cross-Validation RMSE: 0.01</li>
                    <li>Training RMSE: 0.05</li>
                    <li>Training R² Score: 0.9866</li>
                </ul>
            </details>
            
            <h3>Visualization Comparisons:</h3>
            <div class="visualization-container">

                <!-- Best Actual vs Predicted Sale Prices -->
                <div class="image-text-row">
                    <img src="stacked_visualizations/stacked_actual_vs_predicted.png" alt="Best Actual vs Predicted: Stacked Residual Model">
                    <div class="text-content">
                        <p><strong>Best Actual vs Predicted:</strong> The Stacked Residual Model delivers the most accurate alignment with the red dashed line, broadcasting reliable predictions for all houses at all price ranges. Deviations are minimal compared to other models, even for higher-priced homes. This is further proven by the RMSE being 0.9866, presenting the model's accuracy.</p>
                    </div>
                </div>

                <!-- Best Residual Distribution -->
                <div class="image-text-row">
                    <img src="forest_visualizations/rf_residuals.png" alt="Best Residual Distribution: Random Forest">
                    <div class="text-content">
                        <p><strong>Best Residual Distribution:</strong> The Random Forest Model has the tightest residual distribution centering, showcasing its ability to minimize errors effectively. The symmetrical and narrow spread indicates both precise predictions with low variance.</p>
                    </div>
                </div>

                <!-- Best Cross-Validation RMSE -->
                <div class="image-text-row">
                    <img src="ridge_visualizations/ridge_cv_rmse.png" alt="Best Cross-Validation RMSE: Ridge Regression">
                    <div class="text-content">
                        <p><strong>Best Cross-Validation RMSE:</strong> Ridge Regression demonstrates the most consistent RMSE across folds, as shown in the bar plot. This consistency indicates stable performance and robustness across different data splits.</p>
                    </div>
                </div>

                <!-- Best Feature Importance -->
                <div class="image-text-row">
                    <img src="forest_visualizations/rf_top_features.png" alt="Best Feature Importance: Random Forest">
                    <div class="text-content">
                        <p><strong>Best Feature Importance:</strong> The Random Forest Model provides a clear and interpretable bar plot, highlighting "Overall Quality" and "GrLivArea" as dominant features. Further, the features are dominantly found to be the most siginifican, indicating confidence from the model.</p>
                    </div>
                </div>

                <!-- Best Hyperparameter Tuning Visualization -->
                <div class="image-text-row">
                    <img src="lasso_visualizations/lasso_alpha_vs_rmse.png" alt="Best Hyperparameter Tuning Visualization: Lasso Regression">
                    <div class="text-content">
                        <p><strong>Best Hyperparameter Tuning Visualization:</strong> The Lasso Regression plot clearly illustrates the relationship between alpha values (on a log scale) and RMSE, highlighting the optimal alpha value. The smooth curve provides an intuitive understanding of the regularization trade-offs.</p>
                    </div>
                </div>

            </div>

            <h3>Statistic Comparisions:</h3>
            <ul>
                <li><strong>Best Training RMSE:</strong> 0.05 (Achieved by both Random Forest and Stacked Residual Model)</li>
                <li><strong>Highest Training R² Score:</strong> 0.9866 (Achieved by Stacked Residual Model)</li>
                <li><strong>Lowest Cross-Validation RMSE:</strong> 0.01 (Achieved by Ridge, Lasso, and Stacked Residual Models)</li>
                <li><strong>Most Consistent Model:</strong> Ridge Regression (High R² and stable RMSE across folds)</li>
                <li><strong>Most Influential Feature:</strong> "Overall Quality" (Identified as the most important feature across all models)</li>
            </ul>

            <h3>Results:</h3>

            <p> From the midterm, we had noticed heteroskedasticity; decreased accuracy at higher values (evident
                in the legacy model.) To imrpove on this we used a log transformation on the target variable, SalePrice,
                to reduce the skewness and heteroskedasticity. This transformation improved the model's performance
                significantly, as seen in the Lasso regression model. The lasso regression model was strong in the 
                feature selection, as its feature importance plot revealed the key variable of “overall quality”. 
                The Alpha vs RMSE plot also provided an ntuitive visualization of the tradeoffs of regularization, 
                showing the best alpha value that balances accuracy and simplicity in the model. The residuals, while
                centered around zero, were wider than the ride and random forest models that we used, suggesting higher
                predictions errors. This model’s ability to shrink coefficients to zero made it very strong in
                finding the most impactful variables, particularly in high dimensional datasets. However,
                the tradeoff with this is the excessive regularization that can lead to underfitting, when
                some features are penalized unnecessarily and shrunk to become insignificant. Similarly to
                the ridge model, lasso assumes linearity and ends up struggling to model the complex
                relationships between multidimensional datasets. The weaker performance compared to
                the ensemble models we used can be attributed to these two major limitations.
                Computational efficiency and simplicity make it a very strong choice for quick feature
                selection and light modeling, but one of the ensemble models would be best for anything
                beyond that.

                The ridge regression model had its strengths lie in its consistency and stability, as
                demonstrated by the cross validation RMSE plot showing minimal variability across folds.
                The robustness it displays also makes it reliable when using it with unseen data. The Actual
                vs Predicted plot shows good correlation for most of the values but falls apart when it deals
                with extreme prices, its residuals being wider than those of random forest, indicating higher
                errors. Ridge regression also effectively handles the multicollinearity by penalizing large
                coefficients which prevents overfitting but introduces the problem of underfitting similarly
                to lasso regression. The nature of ridge regression is in a linear relationship; therefore, it
                struggles a lot with complex relationships with lots of data. The model’s reliance on feature
                scaling and regularization also means that small hyperparameter errors can lead to
                underfitting. Its performance reflects the limitations stated with regularization preventing
                overfitting and maintaining stability.
                
                
                The random forest model had a very strong and robust performance, with the Actual vs
                Predicted plot showing a strong alignment with the diagonal line, indicating high accuracy
                predictions across the entire range of prices. Its residual distribution was the tightest and
                most symmetrical centered around zero, showing the model’s strength in minimizing errors
                effectively. The model also identified “overall quality” and “GrLivArea” as the most critical
                features, aligning with our knowledge and highlighting the ability to capture complex and
                nonlinear relationships between many variables. We can attribute these strengths to the
                nature of Random Forest, which combines decision trees to reduce overfitting and
                variance. The high-quality performance of this model has the downside of being very
                computationally expensive, especially with larger datasets due to the need to train and
                combine multiple decision trees. Another weakness is the inability to account for feature
                interactions explicitly, differentiating it from the intuitive nature of the feature importance
                plot. The model performed well because of the model’s robustness towards overfitting and
                noise, using ensemble learning to produce stable and accurate predictions.

                The stacked residual model was the best performing model overall. Its Actual vs Predicted
                plot showed the closest alignment to the red dashed line, even for high-priced homes,
                something that several model underperformed in. This model also achieved the highest
                training R2 score of 0.9866 and tied with Random Forest for the best training RMSE of 0.05,
                showing its ability to explain much of the variance in the target variable. The stacked
                residual model combines the strengths of linear models, such as the ridge regression
                model, with the residual error reduction capability of the random forest one, making it
                highly effective at handling both linear and nonlinear patterns. However, due to the
                complexity of this process, the model introduces some tradeoffs: higher sensitivity to
                hyperparameter tuning and increased computational complexity, making it less practical
                for larger and updating datasets that would be used for a more volatile housing market. The
                model is excellent for this dataset due to its hybrid approach that leverages the strengths of
                its components to reduce the residual error and increase the accuracy of the predictions.
                The model’s performance is a testament to the effectiveness of ensemble learning and the
                power of combining different models to create a more robust and accurate prediction
                system.
                
                Some next steps would be to create more visualizations for the stacked model, and perhaps explore other
                ensemble models such as gradient boosting or XGBoost to see if they can improve the model’s performance.
                Furthermore, we could also try different models in the stack, such as a neural network, to see if it can
                improve the model’s performance. We could also try to improve the model’s performance by using different
                feature selection techniques, such as recursive feature elimination, to see if we can improve the model’s
                performance.</p>
            
            </p>
        </section>
        
        <!-- References Section -->
        <section id="references">
            <h2>References</h2>
            <ul>
                <li>Kok, N., Monkkonen, P., & Quigley, J. M. (2014). Land use regulations and the value of land and housing: An intra-metropolitan analysis. <em>Journal of Urban Economics, 81</em>, 136–148.</li>
                <li>Manning, W. G. (1998). The logged dependent variable, heteroscedasticity, and the retransformation problem. <em>Journal of Health Economics, 17</em>(3), 283–295.</li>
                <li>James, G., et al. (2018). <em>Introduction to Statistical Learning</em>. Springer.</li>
            </ul>
        </section>
        
        <!-- Contribution Table Section -->
        <section id="contribution">
            <h2>Contribution Table</h2>
            <table>
                <thead>
                    <tr>
                        <th>Name</th>
                        <th>Contribution to the Midterm Review</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Lohith Dasari</td>
                        <td>Presentation</td>
                    </tr>
                    <tr>
                        <td>Kushal Dudipala</td>
                        <td> Wrote code for lasso regression, organized codebase and github, wrote code for website, updated the visualization/statistics/methods write up section, edited the results write up section, created visualizations, trained models for visualization and quantitative data, README.md/ Statistics.md</td>
                    </tr>
                    <tr>
                        <td>Axel Diaz</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>Ben DiPrete</td>
                        <td>Wrote code for ridge regression, random forest, and stacked model</td>
                    </tr>
                    <tr>
                        <td>Miguel Cruz</td>
                        <td>Updated the Result write up section</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="gantt-chart">
            <h2>Gantt Chart</h2>
            <p>Download the Gantt chart file by clicking the link below:</p>
            <p><a href="ML%20Gantt%20Chart.xlsx" download>Download Gantt Chart</a></p>
        </section>
        
    </div>

</body>
</html>
